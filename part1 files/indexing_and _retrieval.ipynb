{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Public\\AccountPictures\\anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import json\n",
    "import bm25s\n",
    "import gensim.downloader as dl\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and parsing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(filepath):\n",
    "    items = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            items.append(item)\n",
    "    return items\n",
    "\n",
    "docs = load_jsonl(\"docs.jsonl\")      # List of dicts with keys [\"doc_id\", \"text\"]\n",
    "queries = load_jsonl(\"queries.jsonl\")  # List of dicts with (e.g.) [\"query_id\", \"query_text\", \"doc_id\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lexical Indexing with BM25 (sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    }
   ],
   "source": [
    "# Prepare the corpus for BM25\n",
    "corpus = [d[\"text\"] for d in docs]\n",
    "\n",
    "# Initialize BM25 index\n",
    "index = bm25s.BM25()\n",
    "\n",
    "# 1) Tokenize the corpus and retrieve both tokens & dictionary\n",
    "corpus_tokens, dictionary = bm25s.tokenize(corpus)\n",
    "\n",
    "# 2) Build the index using these tokens\n",
    "index.index(corpus_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve top-k documents for a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "def bm25_retrieve(query, k=5):\n",
    "    # Tokenize the query and map tokens using the corpus dictionary\n",
    "    q_tokens = query.split()  # Basic tokenization\n",
    "    ids = [dictionary[t] for t in q_tokens if t in dictionary]  # Filter OOV words\n",
    "\n",
    "    if not ids:\n",
    "        print(f\"No valid tokens found for query: {query}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "    # Retrieve top-k results using BM25 index\n",
    "    results = index.retrieve(query_tokens=[ids], k=k)  # No need to pass `corpus` here\n",
    "\n",
    "    # Extract top-k document indices and scores\n",
    "    doc_indices = results.documents[0]  # First query's results\n",
    "    scores = results.scores[0]  # First query's scores\n",
    "\n",
    "\n",
    "    return [docs[doc_idx]['doc_id'] for doc_idx in doc_indices]\n",
    "\n",
    "\n",
    "\n",
    "for query in queries[:5]:  # Check the first few queries\n",
    "    bm25_retrieve(query[\"query\"], k=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs.jsonl') as f:\n",
    "    docs = [json.loads(line) for line in f]\n",
    "with open('queries.jsonl') as f:\n",
    "    queries = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dense Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized(ids=[[0, 1, 2, 3]], vocab={'example': 0, 'sentence': 1, 'glove': 2, 'encoding': 3})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import bm25s\n",
    "\n",
    "text = \"This is an example sentence for GloVe encoding.\"\n",
    "\n",
    "# Tokenizing with bm25s\n",
    "tokenized = bm25s.tokenize([text])\n",
    "print(tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine static word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained word2vec model\n",
    "model = dl.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding creation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "doc_ids_static = []\n",
    "vecs_static_list = []  # will collect vectors as python lists\n",
    "\n",
    "# Process each document\n",
    "for d in docs:\n",
    "    text = d[\"text\"]\n",
    "    tokens = bm25s.tokenize(text).vocab.keys()\n",
    "    vecs = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            # Get the GloVe vector for each token\n",
    "            vec = model[token]\n",
    "            vecs.append(vec)\n",
    "        except KeyError:\n",
    "            # Skip tokens not in GloVe's vocabulary\n",
    "            pass\n",
    "    \n",
    "    if vecs:\n",
    "        # Average the vectors if valid vectors are found\n",
    "        vecs_static_list.append(np.mean(vecs, axis=0))\n",
    "        doc_ids_static.append(d[\"doc_id\"])\n",
    "    else:\n",
    "        # Fallback to a zero vector if no valid tokens\n",
    "        vecs_static_list.append(np.zeros(300))  \n",
    "        doc_ids_static.append(d[\"doc_id\"])\n",
    "\n",
    "\n",
    "# Convert the list of vectors into a numpy array\n",
    "vecs_static = np.array(vecs_static_list)  \n",
    "\n",
    "print(\"Embedding creation complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_dense_static(query_text, k=10):\n",
    "    tokens = bm25s.tokenize(query_text).vocab.keys()\n",
    "    print(tokens)\n",
    "    vecs = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            # Get the GloVe vector for each token\n",
    "            vec = model[token]\n",
    "            vecs.append(vec)\n",
    "        except KeyError:\n",
    "            # Skip tokens not in GloVe's vocabulary\n",
    "            pass\n",
    "    query_vec = np.mean(vecs, axis=0) if vecs else np.zeros(300)\n",
    "    vecs_norm = np.linalg.norm(vecs_static, axis=1)\n",
    "    \n",
    "    # compute cosine similarity\n",
    "    scores = np.dot(vecs_static, query_vec) / (vecs_norm * np.linalg.norm(query_vec))\n",
    "    topk_doc_ids = np.argsort(scores)[::-1][:k]\n",
    "    return [doc_ids_static[i] for i in topk_doc_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine contextual word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [02:04<00:00, 40.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embedding creation complete!\n",
      "(5000, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModel.from_pretrained(bert_model_name)\n",
    "bert_model.eval()\n",
    "bert_model.cuda()  # if you have GPU; else remove\n",
    "\n",
    "doc_ids_bert = []\n",
    "bert_vecs_list = []\n",
    "\n",
    "# Function to split text into chunks\n",
    "def split_into_chunks(text, tokenizer, max_length):\n",
    "    \"\"\"Splits a long text into chunks of max_length tokens.\"\"\"\n",
    "    words = text.split()  # Split the text into words\n",
    "    chunks = [\" \".join(words[i:i + max_length]) for i in range(0, len(words), max_length)]\n",
    "    return chunks\n",
    "\n",
    "# Process documents\n",
    "for d in tqdm(docs):\n",
    "    text = d[\"text\"]\n",
    "    chunks = split_into_chunks(text, bert_tokenizer, max_length=512)\n",
    "    \n",
    "    chunk_vecs = []\n",
    "    for chunk in chunks:\n",
    "        chunk_encoding = bert_tokenizer(\n",
    "            chunk,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = chunk_encoding[\"input_ids\"].cuda()  # shape (1, seq_len)\n",
    "        attention_mask = chunk_encoding[\"attention_mask\"].cuda()  # shape (1, seq_len)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "            hidden_states = outputs.last_hidden_state[0]  # shape (seq_len, 768)\n",
    "            avg_vec = hidden_states.mean(dim=0)           # shape (768,)\n",
    "        chunk_vecs.append(avg_vec)\n",
    "    \n",
    "    # Average all chunk vectors to get the document vector\n",
    "    doc_avg_vec = torch.stack(chunk_vecs).mean(dim=0)  # shape (768,)\n",
    "    avg_vec_np = doc_avg_vec.cpu().numpy()\n",
    "    \n",
    "    # Store results\n",
    "    doc_ids_bert.append(d[\"doc_id\"])\n",
    "    bert_vecs_list.append(avg_vec_np)\n",
    "\n",
    "# Convert list of vectors to a numpy array\n",
    "bert_vecs = np.array(bert_vecs_list)  # shape = (num_docs, 768)\n",
    "print(\"BERT embedding creation complete!\")\n",
    "print(bert_vecs.shape)\n",
    "\n",
    "# Save the results\n",
    "np.save(open(\"bert_doc_ids.npy\", \"wb\"), np.array(doc_ids_bert))\n",
    "np.save(open(\"bert_vecs.npy\", \"wb\"), bert_vecs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_bert(query_text, k=10):\n",
    "    encoding = bert_tokenizer(\n",
    "        query_text, truncation=True, max_length=512, return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].cuda()\n",
    "    attention_mask = encoding[\"attention_mask\"].cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state[0]\n",
    "        query_vec = hidden_states.mean(dim=0)\n",
    "    query_vec_np = query_vec.cpu().numpy()\n",
    "    \n",
    "    # compute cos similarities with bert_vecs\n",
    "    dot_scores = bert_vecs @ query_vec_np\n",
    "    norm_docs = np.linalg.norm(bert_vecs, axis=1)\n",
    "    norm_query = np.linalg.norm(query_vec_np)\n",
    "    scores = dot_scores / (norm_docs * norm_query + 1e-8)\n",
    "    \n",
    "    topk_indices = np.argsort(-scores)[:k]\n",
    "    topk_doc_ids = [doc_ids_bert[i] for i in topk_indices]\n",
    "    return topk_doc_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a pre-trained text embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = \"sentence-transformers/multi-qa-MiniLM-L6-dot-v1\"\n",
    "st_model = SentenceTransformer(model_name)\n",
    "\n",
    "doc_ids_st = []\n",
    "st_vecs_list = []\n",
    "\n",
    "for d in docs:\n",
    "    text = d[\"text\"]\n",
    "    # encode method handles tokenization internally\n",
    "    emb = st_model.encode(text, normalize_embeddings=False)  # shape: (384,) for that model\n",
    "    doc_ids_st.append(d[\"doc_id\"])\n",
    "    st_vecs_list.append(emb)\n",
    "\n",
    "st_vecs = np.array(st_vecs_list)  # shape = (5000, 384)\n",
    "\n",
    "np.save(open(\"st_doc_ids.npy\", \"wb\"), np.array(doc_ids_st))\n",
    "np.save(open(\"st_vecs.npy\", \"wb\"), st_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_st(query_text, k=10):\n",
    "    q_emb = st_model.encode(query_text, normalize_embeddings=False)\n",
    "    dot_scores = st_vecs @ q_emb\n",
    "    norm_docs = np.linalg.norm(st_vecs, axis=1)\n",
    "    norm_query = np.linalg.norm(q_emb)\n",
    "    scores = dot_scores / (norm_docs * norm_query + 1e-8)\n",
    "    \n",
    "    topk_indices = np.argsort(-dot_scores)[:k]\n",
    "    topk_doc_ids = [doc_ids_st[i] for i in topk_indices]\n",
    "    return topk_doc_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(index_retrieval_fn, queries, k=20):\n",
    "    hits = 0\n",
    "    for q in queries:\n",
    "        relevant_id = q[\"doc_id\"]\n",
    "        topk = index_retrieval_fn(q[\"query\"], k=k)\n",
    "        if relevant_id in topk:\n",
    "            hits += 1\n",
    "    return hits / len(queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(index_retrieval_fn, queries, k=100):\n",
    "    s = 0.0\n",
    "    for q in queries:\n",
    "        relevant_id = q[\"doc_id\"]\n",
    "        topk = index_retrieval_fn(q[\"query\"], k=k)\n",
    "        # find rank\n",
    "        rr = 0.0\n",
    "        for rank, doc_id in enumerate(topk, start=1):\n",
    "            if doc_id == relevant_id:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        s += rr\n",
    "    return s / len(queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['how', 'many', 'episodes', 'chicago', 'fire', 'season'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "first_query = queries[0][\"query\"]\n",
    "bm25_top = bm25_retrieve(first_query, k=10)\n",
    "static_top = retrieve_dense_static(first_query, k=10)\n",
    "bert_top = retrieve_bert(first_query, k=10)\n",
    "st_top = retrieve_st(first_query, k=10)\n",
    "\n",
    "with open(\"q1.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(bm25_top) + \"\\n\")\n",
    "    f.write(\" \".join(static_top) + \"\\n\")\n",
    "    f.write(\" \".join(bert_top) + \"\\n\")\n",
    "    f.write(\" \".join(st_top) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['how', 'many', 'episodes', 'chicago', 'fire', 'season'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'sings', 'love', 'keep', 'us', 'alive', 'eagles'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['nitty', 'gritty', 'dirt', 'band', 'fishin', 'dark', 'album'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'were', 'three', 'elves', 'got', 'rings'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['order', 'prove', 'disparate', 'impact', 'you', 'first', 'must', 'establish'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['where', 'do', 'characters', 'live', 'us'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'makes', 'decisions', 'about', 'what', 'produce', 'market', 'economy'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['when', 'new', 'tappan', 'zee', 'bridge', 'going', 'finished'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'recorded', 'can', 'help', 'falling', 'love', 'you'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'plays', 'doc', 'back', 'future'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['when', 'did', 'stop', 'cigarette', 'advertising', 'television'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'has', 'been', 'chosen', 'brand', 'ambassador', 'campaign', 'beti', 'bachao', 'padhao'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['how', 'many', 'seasons', 'prison', 'break', 'netflix'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['what', 'name', 'atom', 'bomb', 'dropped', 'usa', 'hiroshima'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['when', 'did', 'american', 'two', 'party', 'system', 'began', 'emerge'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['how', 'many', 'lines', 'symmetry', 'equilateral', 'triangle'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['how', 'many', 'seasons', 'oc'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['latest', 'season', 'keeping', 'up', 'kardashians'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['how', 'dose', 'poet', 'present', 'death', 'voyage', 'crossing', 'bar'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'were', 'farmers', 'kept', 'small', 'portion', 'crops', 'gave', 'rest', 'landowners'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['what', 'reasons', 'states', 'impose', 'protectionists', 'policies', 'other', 'countries'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['where', 'building', 'new', 'raiders', 'stadium'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['does', 'joe', 'die', 'purge', 'election', 'year'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'do', 'you', 'meet', 'gates', 'heaven'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['spain', 'second', 'largest', 'country', 'europe'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['difference', 'between', 'single', 'layer', 'perceptron', 'multilayer'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['what', 'meaning', 'dragon', 'boat', 'festival'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'edmund', 'days', 'our', 'lives'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['want', 'you', 'everywhere', 'song'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['what', 'degree', 'crock', 'pot', 'low'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['royal', 'society', 'protection', 'birds', 'number', 'members'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['when', 'does', 'jenny', 'humphrey', 'come', 'back', 'gossip', 'girl'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['when', 'did', 'athens', 'emerges', 'wealthiest', 'greek', 'city', 'state'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['why', 'does', 'king', 'from', 'tekken', 'wear', 'mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'sings', 'war', 'don', 'let', 'me', 'down'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'invented', 'frisbee', 'how', 'did', 'get', 'its', 'name'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'plays', 'orange', 'new', 'black'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['how', 'long', 'prime', 'minister', 'stay', 'office', 'canada'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['when', 'do', 'willow', 'tara', 'get', 'back', 'together'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'performed', 'first', 'section', '1794'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'sang', 'movie', 'walk', 'line'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['where', 'did', 'aeneas', 'go', 'when', 'he', 'left', 'carthage'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'involved', 'selling', 'product', 'mutual', 'fund', 'insurance'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['where', 'capital', 'habsburg', 'empire', 'located'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'plays', 'general', 'hux', 'last', 'jedi'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['what', 'happened', 'brother', 'accountant'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['when', 'were', 'winnie', 'pooh', 'books', 'written'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'wrote', 'theme', 'song', 'mission', 'impossible'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['what', 'point', 'having', 'belly', 'button'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['where', 'slide', 'placed', 'microscope'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['how', 'many', 'episodes', 'chicago', 'fire', 'season'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'sings', 'love', 'keep', 'us', 'alive', 'eagles'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['nitty', 'gritty', 'dirt', 'band', 'fishin', 'dark', 'album'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'were', 'three', 'elves', 'got', 'rings'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['order', 'prove', 'disparate', 'impact', 'you', 'first', 'must', 'establish'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['where', 'do', 'characters', 'live', 'us'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'makes', 'decisions', 'about', 'what', 'produce', 'market', 'economy'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['when', 'new', 'tappan', 'zee', 'bridge', 'going', 'finished'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'recorded', 'can', 'help', 'falling', 'love', 'you'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'plays', 'doc', 'back', 'future'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['when', 'did', 'stop', 'cigarette', 'advertising', 'television'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'has', 'been', 'chosen', 'brand', 'ambassador', 'campaign', 'beti', 'bachao', 'padhao'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['how', 'many', 'seasons', 'prison', 'break', 'netflix'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['what', 'name', 'atom', 'bomb', 'dropped', 'usa', 'hiroshima'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['when', 'did', 'american', 'two', 'party', 'system', 'began', 'emerge'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['how', 'many', 'lines', 'symmetry', 'equilateral', 'triangle'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['how', 'many', 'seasons', 'oc'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['latest', 'season', 'keeping', 'up', 'kardashians'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['how', 'dose', 'poet', 'present', 'death', 'voyage', 'crossing', 'bar'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'were', 'farmers', 'kept', 'small', 'portion', 'crops', 'gave', 'rest', 'landowners'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['what', 'reasons', 'states', 'impose', 'protectionists', 'policies', 'other', 'countries'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['where', 'building', 'new', 'raiders', 'stadium'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['does', 'joe', 'die', 'purge', 'election', 'year'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'do', 'you', 'meet', 'gates', 'heaven'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['spain', 'second', 'largest', 'country', 'europe'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['difference', 'between', 'single', 'layer', 'perceptron', 'multilayer'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['what', 'meaning', 'dragon', 'boat', 'festival'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'edmund', 'days', 'our', 'lives'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['want', 'you', 'everywhere', 'song'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['what', 'degree', 'crock', 'pot', 'low'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['royal', 'society', 'protection', 'birds', 'number', 'members'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['when', 'does', 'jenny', 'humphrey', 'come', 'back', 'gossip', 'girl'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['when', 'did', 'athens', 'emerges', 'wealthiest', 'greek', 'city', 'state'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['why', 'does', 'king', 'from', 'tekken', 'wear', 'mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'sings', 'war', 'don', 'let', 'me', 'down'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'invented', 'frisbee', 'how', 'did', 'get', 'its', 'name'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'plays', 'orange', 'new', 'black'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['how', 'long', 'prime', 'minister', 'stay', 'office', 'canada'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['when', 'do', 'willow', 'tara', 'get', 'back', 'together'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'performed', 'first', 'section', '1794'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'sang', 'movie', 'walk', 'line'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['where', 'did', 'aeneas', 'go', 'when', 'he', 'left', 'carthage'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'involved', 'selling', 'product', 'mutual', 'fund', 'insurance'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['where', 'capital', 'habsburg', 'empire', 'located'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'plays', 'general', 'hux', 'last', 'jedi'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['what', 'happened', 'brother', 'accountant'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['when', 'were', 'winnie', 'pooh', 'books', 'written'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['who', 'wrote', 'theme', 'song', 'mission', 'impossible'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['what', 'point', 'having', 'belly', 'button'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['where', 'slide', 'placed', 'microscope'])\n"
     ]
    }
   ],
   "source": [
    "# BM25\n",
    "r_bm25 = recall_at_k(bm25_retrieve, queries, k=20)\n",
    "m_bm25 = mrr(bm25_retrieve, queries, k=200)\n",
    "\n",
    "# Static\n",
    "r_static = recall_at_k(retrieve_dense_static, queries, k=20)\n",
    "m_static = mrr(retrieve_dense_static, queries, k=200)\n",
    "\n",
    "# BERT\n",
    "r_bert = recall_at_k(retrieve_bert, queries, k=20)\n",
    "m_bert = mrr(retrieve_bert, queries, k=200)\n",
    "\n",
    "# Sentence-Transformers\n",
    "r_st = recall_at_k(retrieve_st, queries, k=20)\n",
    "m_st = mrr(retrieve_st, queries, k=200)\n",
    "\n",
    "with open(\"scores.txt\", \"w\") as f:\n",
    "    f.write(f\"{r_bm25} {m_bm25}\\n\")\n",
    "    f.write(f\"{r_static} {m_static}\\n\")\n",
    "    f.write(f\"{r_bert} {m_bert}\\n\")\n",
    "    f.write(f\"{r_st} {m_st}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
