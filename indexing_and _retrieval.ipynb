{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import bm25s\n",
    "import gensim.downloader as dl\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and parsing the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(filepath):\n",
    "    items = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            items.append(item)\n",
    "    return items\n",
    "\n",
    "docs = load_jsonl(\"docs.jsonl\")      # List of dicts with keys [\"doc_id\", \"text\"]\n",
    "queries = load_jsonl(\"queries.jsonl\")  # List of dicts with (e.g.) [\"query_id\", \"query_text\", \"doc_id\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lexical Indexing with BM25 (sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    }
   ],
   "source": [
    "# Prepare the corpus for BM25\n",
    "corpus = [d[\"text\"] for d in docs]\n",
    "\n",
    "# Initialize BM25 index\n",
    "index = bm25s.BM25()\n",
    "\n",
    "# 1) Tokenize the corpus and retrieve both tokens & dictionary\n",
    "corpus_tokens, dictionary = bm25s.tokenize(corpus)\n",
    "\n",
    "# 2) Build the index using these tokens\n",
    "index.index(corpus_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve top-k documents for a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "def bm25_retrieve(query, k=5):\n",
    "    # Tokenize the query and map tokens using the corpus dictionary\n",
    "    q_tokens = query.split()  # Basic tokenization\n",
    "    ids = [dictionary[t] for t in q_tokens if t in dictionary]  # Filter OOV words\n",
    "\n",
    "    if not ids:\n",
    "        print(f\"No valid tokens found for query: {query}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "    # Retrieve top-k results using BM25 index\n",
    "    results = index.retrieve(query_tokens=[ids], k=k)  # No need to pass `corpus` here\n",
    "\n",
    "    # Extract top-k document indices and scores\n",
    "    doc_indices = results.documents[0]  # First query's results\n",
    "    scores = results.scores[0]  # First query's scores\n",
    "\n",
    "\n",
    "    return [docs[doc_idx]['doc_id'] for doc_idx in doc_indices]\n",
    "\n",
    "\n",
    "\n",
    "for query in queries[:5]:  # Check the first few queries\n",
    "    bm25_retrieve(query[\"query\"], k=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs.jsonl') as f:\n",
    "    docs = [json.loads(line) for line in f]\n",
    "with open('queries.jsonl') as f:\n",
    "    queries = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dense Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained word2vec model\n",
    "model = dl.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized(ids=[[0, 1, 2, 3]], vocab={'example': 0, 'sentence': 1, 'glove': 2, 'encoding': 3})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import bm25s\n",
    "\n",
    "text = \"This is an example sentence for GloVe encoding.\"\n",
    "\n",
    "# Tokenizing with bm25s\n",
    "tokenized = bm25s.tokenize([text])\n",
    "print(tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine static word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding creation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "doc_ids_static = []\n",
    "vecs_static_list = []  # will collect vectors as python lists\n",
    "\n",
    "# Process each document\n",
    "for d in docs:\n",
    "    text = d[\"text\"]\n",
    "    tokens = bm25s.tokenize(text).vocab.keys()\n",
    "    vecs = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            # Get the GloVe vector for each token\n",
    "            vec = model[token]\n",
    "            vecs.append(vec)\n",
    "        except KeyError:\n",
    "            # Skip tokens not in GloVe's vocabulary\n",
    "            pass\n",
    "    \n",
    "    if vecs:\n",
    "        # Average the vectors if valid vectors are found\n",
    "        vecs_static_list.append(np.mean(vecs, axis=0))\n",
    "        doc_ids_static.append(d[\"doc_id\"])\n",
    "    else:\n",
    "        # Fallback to a zero vector if no valid tokens\n",
    "        vecs_static_list.append(np.zeros(200))  # Adjusted to 200 dimensions\n",
    "        doc_ids_static.append(d[\"doc_id\"])\n",
    "\n",
    "# Convert the list of vectors into a numpy array\n",
    "vecs_static = np.array(vecs_static_list)  # shape = (len(docs), 200)\n",
    "\n",
    "print(\"Embedding creation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 300)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs_static.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_dense_static(query_text, k=10):\n",
    "    tokens = bm25s.tokenize(query_text).vocab.keys()\n",
    "    vecs = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        try:\n",
    "            # Get the GloVe vector for each token\n",
    "            vec = model[token]\n",
    "            vecs.append(vec)\n",
    "        except KeyError:\n",
    "            # Skip tokens not in GloVe's vocabulary\n",
    "            pass\n",
    "    query_vec = np.mean(vecs, axis=0) if vecs else np.zeros(200)\n",
    "    vecs_norm = np.linalg.norm(vecs_static, axis=1)\n",
    "    \n",
    "    # compute cosine similarity\n",
    "    scores = np.dot(vecs_static, query_vec) / (vecs_norm * np.linalg.norm(query_vec))\n",
    "    topk_doc_ids = np.argsort(scores)[::-1][:k]\n",
    "    return [doc_ids_static[i] for i in topk_doc_ids]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine contextual word-vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 5000/5000 [02:06<00:00, 39.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embedding creation complete!\n",
      "(5000, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "bert_model_name = \"roberta-base\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModel.from_pretrained(bert_model_name)\n",
    "bert_model.eval()\n",
    "bert_model.cuda()  # if you have GPU; else remove\n",
    "\n",
    "doc_ids_bert = []\n",
    "bert_vecs_list = []\n",
    "\n",
    "for d in tqdm(docs):\n",
    "    text = d[\"text\"]\n",
    "    encoding = bert_tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].cuda()  # if GPU\n",
    "    attention_mask = encoding[\"attention_mask\"].cuda() \n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "        # outputs.last_hidden_state shape = (batch_size, seq_len, hidden_dim=768)\n",
    "        hidden_states = outputs.last_hidden_state[0]  # shape (seq_len, 768)\n",
    "        avg_vec = hidden_states.mean(dim=0)           # shape (768,)\n",
    "    avg_vec_np = avg_vec.cpu().numpy()\n",
    "    doc_ids_bert.append(d[\"doc_id\"])\n",
    "    bert_vecs_list.append(avg_vec_np)\n",
    "\n",
    "bert_vecs = np.array(bert_vecs_list)  # shape = (5000, 768)\n",
    "print(\"BERT embedding creation complete!\")\n",
    "print(bert_vecs.shape)\n",
    "\n",
    "np.save(open(\"bert_doc_ids.npy\", \"wb\"), np.array(doc_ids_bert))\n",
    "np.save(open(\"bert_vecs.npy\", \"wb\"), bert_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_bert(query_text, k=10):\n",
    "    encoding = bert_tokenizer(\n",
    "        query_text, truncation=True, max_length=512, return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].cuda()\n",
    "    attention_mask = encoding[\"attention_mask\"].cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state[0]\n",
    "        query_vec = hidden_states.mean(dim=0)\n",
    "    query_vec_np = query_vec.cpu().numpy()\n",
    "    \n",
    "    # compute cos similarities with bert_vecs\n",
    "    dot_scores = bert_vecs @ query_vec_np\n",
    "    norm_docs = np.linalg.norm(bert_vecs, axis=1)\n",
    "    norm_query = np.linalg.norm(query_vec_np)\n",
    "    scores = dot_scores / (norm_docs * norm_query + 1e-8)\n",
    "    \n",
    "    topk_indices = np.argsort(-scores)[:k]\n",
    "    topk_doc_ids = [doc_ids_bert[i] for i in topk_indices]\n",
    "    return topk_doc_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a pre-trained text embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = \"sentence-transformers/multi-qa-MiniLM-L6-dot-v1\"\n",
    "st_model = SentenceTransformer(model_name)\n",
    "\n",
    "doc_ids_st = []\n",
    "st_vecs_list = []\n",
    "\n",
    "for d in docs:\n",
    "    text = d[\"text\"]\n",
    "    # encode method handles tokenization internally\n",
    "    emb = st_model.encode(text, normalize_embeddings=False)  # shape: (384,) for that model\n",
    "    doc_ids_st.append(d[\"doc_id\"])\n",
    "    st_vecs_list.append(emb)\n",
    "\n",
    "st_vecs = np.array(st_vecs_list)  # shape = (5000, 384)\n",
    "\n",
    "np.save(open(\"st_doc_ids.npy\", \"wb\"), np.array(doc_ids_st))\n",
    "np.save(open(\"st_vecs.npy\", \"wb\"), st_vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_st(query_text, k=10):\n",
    "    q_emb = st_model.encode(query_text, normalize_embeddings=False)\n",
    "    dot_scores = st_vecs @ q_emb\n",
    "    norm_docs = np.linalg.norm(st_vecs, axis=1)\n",
    "    norm_query = np.linalg.norm(q_emb)\n",
    "    scores = dot_scores / (norm_docs * norm_query + 1e-8)\n",
    "    \n",
    "    topk_indices = np.argsort(-dot_scores)[:k]\n",
    "    topk_doc_ids = [doc_ids_st[i] for i in topk_indices]\n",
    "    return topk_doc_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(index_retrieval_fn, queries, k=20):\n",
    "    hits = 0\n",
    "    for q in queries:\n",
    "        relevant_id = q[\"doc_id\"]\n",
    "        topk = index_retrieval_fn(q[\"query\"], k=k)\n",
    "        if relevant_id in topk:\n",
    "            hits += 1\n",
    "    return hits / len(queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(index_retrieval_fn, queries, k=100):\n",
    "    s = 0.0\n",
    "    for q in queries:\n",
    "        relevant_id = q[\"doc_id\"]\n",
    "        topk = index_retrieval_fn(q[\"query\"], k=k)\n",
    "        # find rank\n",
    "        rr = 0.0\n",
    "        for rank, doc_id in enumerate(topk, start=1):\n",
    "            if doc_id == relevant_id:\n",
    "                rr = 1.0 / rank\n",
    "                break\n",
    "        s += rr\n",
    "    return s / len(queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "first_query = queries[0][\"query\"]\n",
    "bm25_top = bm25_retrieve(first_query, k=10)\n",
    "static_top = retrieve_dense_static(first_query, k=10)\n",
    "bert_top = retrieve_bert(first_query, k=10)\n",
    "st_top = retrieve_st(first_query, k=10)\n",
    "\n",
    "with open(\"q1.txt\", \"w\") as f:\n",
    "    f.write(\" \".join(bm25_top) + \"\\n\")\n",
    "    f.write(\" \".join(static_top) + \"\\n\")\n",
    "    f.write(\" \".join(bert_top) + \"\\n\")\n",
    "    f.write(\" \".join(st_top) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    }
   ],
   "source": [
    "# BM25\n",
    "r_bm25 = recall_at_k(bm25_retrieve, queries, k=20)\n",
    "m_bm25 = mrr(bm25_retrieve, queries, k=20)\n",
    "\n",
    "# Static\n",
    "r_static = recall_at_k(retrieve_dense_static, queries, k=20)\n",
    "m_static = mrr(retrieve_dense_static, queries, k=20)\n",
    "\n",
    "# BERT\n",
    "r_bert = recall_at_k(retrieve_bert, queries, k=20)\n",
    "m_bert = mrr(retrieve_bert, queries, k=20)\n",
    "\n",
    "# Sentence-Transformers\n",
    "r_st = recall_at_k(retrieve_st, queries, k=20)\n",
    "m_st = mrr(retrieve_st, queries, k=20)\n",
    "\n",
    "with open(\"scores.txt\", \"w\") as f:\n",
    "    f.write(f\"{r_bm25} {m_bm25}\\n\")\n",
    "    f.write(f\"{r_static} {m_static}\\n\")\n",
    "    f.write(f\"{r_bert} {m_bert}\\n\")\n",
    "    f.write(f\"{r_st} {m_st}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
